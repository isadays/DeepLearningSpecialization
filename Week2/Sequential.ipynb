import torch
import torch.nn as nn
from transformers import BertModel

class RobustBertClassifier(nn.Module):
    def __init__(self, model_name, hidden_sizes=[256, 128], dropout_prob=0.3):
        """
        Args:
            model_name (str): Name or path to the pre-trained BERT model.
            hidden_sizes (list): List containing the sizes of the hidden layers.
            dropout_prob (float): Dropout probability for regularization.
        """
        super(RobustBertClassifier, self).__init__()
        # Load pre-trained BERT model (BERTimbau in Portuguese for example)
        self.bert = BertModel.from_pretrained(model_name)
        
        # Get hidden size from the pre-trained model config
        bert_hidden_size = self.bert.config.hidden_size
        
        # Build a sequential classifier on top of BERT's pooled output ([CLS] token)
        self.classifier = nn.Sequential(
            nn.Linear(bert_hidden_size, hidden_sizes[0]),
            nn.Tanh(),
            nn.Dropout(dropout_prob),
            nn.Linear(hidden_sizes[0], hidden_sizes[1]),
            nn.Tanh(),
            nn.Dropout(dropout_prob),
            nn.Linear(hidden_sizes[1], 1)  # Output layer for binary classification
        )
        
    def forward(self, input_ids, attention_mask):
        # Get outputs from BERT
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output  # [CLS] token representation
        
        # Pass through the custom classifier network
        logits = self.classifier(pooled_output)
        return logits

# Example usage:
if __name__ == '__main__':
    from transformers import BertTokenizer

    MODEL_NAME = "neuralmind/bert-base-portuguese-cased"  # or another suitable model
    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)

    # Sample text
    texts = ["Exemplo de transcrição de chamada de cliente."]
    encodings = tokenizer(
        texts,
        padding='max_length',
        truncation=True,
        max_length=200,
        return_tensors='pt'
    )

    # Instantiate the model
    model = RobustBertClassifier(model_name=MODEL_NAME, hidden_sizes=[256, 128], dropout_prob=0.3)
    model.eval()  # Set the model to evaluation mode

    # Forward pass (for inference)
    input_ids = encodings['input_ids']
    attention_mask = encodings['attention_mask']
    with torch.no_grad():
        logits = model(input_ids=input_ids, attention_mask=attention_mask)
        # Convert logits to probabilities using sigmoid (since this is binary classification)
        probabilities = torch.sigmoid(logits)

    # Obtain binary predictions (threshold 0.5)
    predictions = (probabilities >= 0.5).long()

    print("Logits:", logits)
    print("Probabilities:", probabilities)
    print("Predictions:", predictions)