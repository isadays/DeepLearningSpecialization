import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
from sklearn.preprocessing import LabelEncoder, StandardScaler
from torch.utils.data import Dataset, DataLoader

# ========== 1. Define Dataset Class ==========
class PropensityDataset(Dataset):
    def __init__(self, df, categorical_cols, continuous_cols, target_col):
        self.encoders = {col: LabelEncoder().fit(df[col]) for col in categorical_cols}

        # Encode categorical variables
        self.categorical_data = [torch.tensor(self.encoders[col].transform(df[col]), dtype=torch.long) for col in categorical_cols]
        self.categorical_data = torch.stack(self.categorical_data, dim=1)  # Convert to tensor of shape (N, num_cats)

        # Normalize continuous variables
        self.scaler = StandardScaler().fit(df[continuous_cols])
        self.continuous_data = torch.tensor(self.scaler.transform(df[continuous_cols]), dtype=torch.float32)

        # Target variable
        self.target = torch.tensor(df[target_col].values, dtype=torch.float32).unsqueeze(1)

    def __len__(self):
        return len(self.target)

    def __getitem__(self, idx):
        return self.categorical_data[idx], self.continuous_data[idx], self.target[idx]

# ========== 2. Define the Transformer Model ==========
class PropensityTransformer(nn.Module):
    def __init__(self, categorical_sizes, embedding_dim, num_cont_features, hidden_dim, num_heads, num_layers):
        super(PropensityTransformer, self).__init__()

        # Embedding layers for categorical variables
        self.embeddings = nn.ModuleList([nn.Embedding(cat_size, embedding_dim) for cat_size in categorical_sizes])

        # Linear layer for continuous features
        self.continuous_linear = nn.Linear(num_cont_features, embedding_dim)

        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=hidden_dim)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # Final classifier
        self.fc = nn.Linear(embedding_dim, 1)
        self.sigmoid = nn.Sigmoid()  # Output probability

    def forward(self, categorical_data, continuous_data):
        # Pass categorical data through embeddings and concatenate
        cat_embeddings = [embed(cat_data) for embed, cat_data in zip(self.embeddings, categorical_data.T)]
        cat_embeddings = torch.cat(cat_embeddings, dim=1)

        # Process continuous features
        cont_features = self.continuous_linear(continuous_data)

        # Combine categorical embeddings with continuous features
        combined_features = cat_embeddings + cont_features  # Element-wise sum to align dimensions

        # Pass through transformer
        transformer_output = self.transformer(combined_features.unsqueeze(1))  # Add sequence dimension

        # Classification
        output = self.fc(transformer_output.squeeze(1))  # Remove sequence dim
        return self.sigmoid(output)

# ========== 3. Data Preparation ==========
# Example: Dummy dataset
data = pd.DataFrame({
    "customer_id": [101, 102, 103, 104, 105],
    "channel_type": ["SMS", "Email", "Call", "SMS", "Email"],
    "transaction_amount": [200.5, 500.1, 150.2, 300.0, 450.7],
    "num_previous_interactions": [3, 10, 2, 5, 8],
    "target": [1, 0, 1, 0, 1]
})

categorical_cols = ["customer_id", "channel_type"]
continuous_cols = ["transaction_amount", "num_previous_interactions"]
target_col = "target"

# Create dataset and dataloader
dataset = PropensityDataset(data, categorical_cols, continuous_cols, target_col)
dataloader = DataLoader(dataset, batch_size=2, shuffle=True)

# Define categorical sizes for embedding
categorical_sizes = [len(dataset.encoders[col].classes_) for col in categorical_cols]

# ========== 4. Model Initialization ==========
embedding_dim = 16
hidden_dim = 64
num_heads = 4
num_layers = 2
num_cont_features = len(continuous_cols)

model = PropensityTransformer(categorical_sizes, embedding_dim, num_cont_features, hidden_dim, num_heads, num_layers)

# Loss function and optimizer
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# ========== 5. Training Loop ==========
epochs = 20
for epoch in range(epochs):
    model.train()
    total_loss = 0

    for cat_data, cont_data, targets in dataloader:
        optimizer.zero_grad()

        # Forward pass
        outputs = model(cat_data, cont_data)

        # Compute loss
        loss = criterion(outputs, targets)
        total_loss += loss.item()

        # Backpropagation
        loss.backward()
        optimizer.step()

    print(f"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(dataloader):.4f}")

# ========== 6. Inference ==========
model.eval()
with torch.no_grad():
    for cat_data, cont_data, targets in dataloader:
        preds = model(cat_data, cont_data)
        print("Predicted Probabilities:", preds.squeeze().numpy())